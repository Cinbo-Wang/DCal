% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DCal.R
\name{DCal.mean_treat}
\alias{DCal.mean_treat}
\title{Estimation of E[Y(1)] or E[Y(0)] from observational data using Double Calibration Estimator.}
\usage{
DCal.mean_treat(
  X,
  Y,
  W,
  Y.family = c("gaussian", "binomial")[1],
  B = 3,
  is.scale = TRUE,
  r1 = NULL,
  pi = NULL,
  alpha = 0.9,
  is.parallel = FALSE,
  core_num = NULL
)
}
\arguments{
\item{X}{The n by p input covariance matrix.}

\item{Y}{The n dimensional observed response.}

\item{W}{The n dimensional binary vector indicating treatment assignment.}

\item{Y.family}{Response type. A character string representing one of the built-in families, including 'gaussion' and 'binomial'.}

\item{B}{The number of iterations for random splits for cross-fitting.}

\item{is.scale}{Whether or not to scale the input covariance matrix.}

\item{r1}{Optional n dimensional vector of an initial estimate of E[Y_i(1) |
X_i] for i = 1, ..., n. The default is NULL.}

\item{pi}{Optional n dimensional vector of an initial estimate of E[W_i |
X_i] for i = 1, ..., n. The default is NULL.}

\item{alpha}{The elastic net mixing parameter, with \(0 \eqn{\leq} alpha \eqn{\leq} 1\).}

\item{is.parallel}{Whether to perform parallel computation. Default is False.}

\item{core_num}{Number of cores used for parallel computation.}
}
\value{
\item{ATE_sc}{Single calibation estimation of E[Y(1)] or E[Y(0)] averaged over B random splits.}
\item{ATE_sc_var}{Estimated variance of the ATE_sc averaged over B random splits.}
\item{ATE_dc}{Double calibation estimation of E[Y(1)] or E[Y(0)] averaged over B random splits.}
\item{ATE_dc_var}{Estimated variance of the ATE_dc averaged over B random splits.}
\item{ATE_mat}{A matrix containing ATE_sc, ATE_sc_var, ATE_dc and ATE_dc_var in each random split.}
}
\description{
Estimation of E[Y(1)] or E[Y(0)] from observational data using Double Calibration Estimator.
}
\examples{
\dontrun{
# Sparse OR, dense nonlinear PS--------
p = 300; s_or = 10; n = 100;rho=0.9;rd_num = 1
Sigma_X <- matrix(0,p,p)
for(i in 1:p){
  for(j in 1:p){
    Sigma_X[i,j] <- rho ** abs(i-j)
  }
}

X <- MASS::mvrnorm(n=n,mu=rep(0,p),Sigma = Sigma_X)
# dense propensity model
Xf <- X[,1:4]
Xf[,1] <- exp(0.5*X[,1])
Xf[,2] <- 10 + X[,2]/(1+exp(X[,1]))
Xf[,3] <- (0.05*X[,1]*X[,3]+0.6)**2
Xf[,4] <- (X[,2]+X[,4]+10)**2

gamma_true <- rep(0,p)
for(j in 1:p){
  gamma_true[j] <-  1/j
}
gamma_true <- gamma_true / norm(gamma_true,type='2')

lp <- scale(Xf[,1:4]) \%*\% c(1,-1/2,1/4,-1/8) + X \%*\% gamma_true
pi_W <- 1/(1+exp(-lp))
pi_W <- pmin(pmax(pi_W, 0.05), 0.95)
W <- rbinom(n = n,size=1,p=pi_W)

# sparse linear OR
beta_true <- rep(0,p)
act_loc <- 1:d_beta # Confounder
beta_true[act_loc] <- runif(d_beta,1,2)*sample(c(1,-1),size=d_beta,replace = T)
beta_true <-  beta_true / norm(beta_true,type='2')
potential_outcome_treat <- X \%*\% beta_true + 1
potential_outcome_control <- X \%*\% beta_true
Y <- potential_outcome_treat*W + potential_outcome_control*(1-W) + rnorm(n,0,1)
tau_treat <- mean(potential_outcome_treat)
mean_treat_cf_dcal_ls <- cf.DCal.mean_treat(X,Y,W,B=3,r1 = NULL,pi = NULL,
                     is.scale = T,Y.family = 'gaussian',alpha = 0.9, is.parallel=F,core_num=3)

mean_treat_cf_dcal_ls
}
}
\references{
Lin Liu, and Yuhao Wang. (2023) \emph{Root-n consistent semiparametric learning with
high-dimensional nuisance functions under minimal sparsity.} \url{https://doi.org/10.48550/arXiv.2305.04174}
}
\author{
Xinbo Wang
}
